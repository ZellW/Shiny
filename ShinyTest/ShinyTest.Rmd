---
title: "profvis Examples"
output:
  rmdformats::readthedown:
    highlight: pygments
    code_folding: show
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
if(!require(easypackages)){install.packages("easypackages")}
library(easypackages)
packages("profvis", "shiny", prompt = FALSE)

setwd("~/GitHub/Shiny/ShinyTest")
```

See https://www.rstudio.com/resources/videos/profiling-and-performance/

# Example 1

```{r eval=FALSE}
library(profvis)

# Generate data
times <- 4e5
cols <- 150
data <- as.data.frame(x = matrix(rnorm(times * cols, mean = 5), ncol = cols))
data <- cbind(id = paste0("g", seq_len(times)), data)

profvis({
     data1 <- data   # Store in another variable for this run
     
     # Get column means
     means <- apply(data1[, names(data1) != "id"], 2, mean)
     
     # Subtract mean from each column
     for (i in seq_along(means)) {
          data1[, names(data1) != "id"][, i] <- data1[, names(data1) != "id"][, i] - means[i]
     }
}, height = "400px")
```

```{r, out.width = "800px", echo=FALSE}
knitr::include_graphics("images/profvis1.jpg")
```

Most of the time is spent in the apply call, so that’s the best candidate for a first pass at optimization. We can also see that the apply results in a lot of memory being allocated and deallocated. Memory “churning” is often a cause for performance problems.

Looking at the flame graph, we can see that apply calls `as.matrix` and `aperm`. These two functions convert the data frame to a matrix and transpose it – so even before we’ve done any useful computations, we’ve spent a large amount of time transforming the data.

We could try to speed this up in a number of ways. One possibility is that we could simply leave the data in matrix form (instead of putting it in a data frame in line 4). That would remove the need for the `as.matrix` call, but it would still require `aperm` to transpose the data. It would also lose the connection of each row to the id column, which is undesirable. In any case, using apply over columns looks like it will be expensive because of the call to aperm.

An obvious alternative is to use `colMeans`. But there’s also another possibility. Data frames are implemented as lists of vectors, where each column is one vector, so we could use lapply or vapply to apply the mean function over each column. Let’s compare the speed of these four different ways of getting column means.

```{r eval=FALSE}
profvis({
  data1 <- data
  # Four different ways of getting column means
  means <- apply(data1[, names(data1) != "id"], 2, mean)
  means <- colMeans(data1[, names(data1) != "id"])
  means <- lapply(data1[, names(data1) != "id"], mean)
  means <- vapply(data1[, names(data1) != "id"], mean, numeric(1))
})
```

```{r, out.width = "800px", echo=FALSE}
knitr::include_graphics("images/profvis2.jpg")
```

`colMeans` is about 6x faster than using apply with `mean`, but it looks like it’s still using `as.matrix`, which takes a significant amount of time.  `lapply/vapply` are faster yet – about 10x faster than `apply`. `lapply` returns the values in a list, while `vapply` returns the values in a numeric vector, which is the form that we want, so it looks like `vapply` is the way to go for this part.

You can also see that the faster methods also result in less memory allocation and deallocation. This is not a coincidence – allocating and deallocating memory can be expensive.

Let’s take the original code and replace `apply` with `vapply`:

```{r eval=FALSE}
profvis({
  data1 <- data
  means <- vapply(data1[, names(data1) != "id"], mean, numeric(1))

  for (i in seq_along(means)) {
    data1[, names(data1) != "id"][, i] <- data1[, names(data1) != "id"][, i] - means[i]
  }
})
```

```{r, out.width = "800px", echo=FALSE}
knitr::include_graphics("images/profvis3.jpg")
```

Our code is about 3x faster than the original version. Most of the time is now spent on line 6, and the majority of that is in the `[<-` function. This is usually called with syntax `x[i, j] <- y`, which is equivalent to `[<- (x, i, j, y)`. In addition to being slow, the code is ugly: on each side of the assignment operator we’re indexing into `data1` twice with `[.`

In this case, it’s useful to take a step back and think about the broader problem. We want to normalize each column. Couldn’t we we apply a function over the columns that does both steps, taking the `mean` and subtracting it? Because a data frame is a list, and we want to assign a list of values into the data frame, we’ll need to use `lapply`.

```{r eval=FALSE}
profvis({
  data1 <- data

  # Given a column, normalize values and return them
  col_norm <- function(col) {
    col - mean(col)
  }

  # Apply the normalizer function over all columns except id
  data1[, names(data1) != "id"] <- lapply(data1[, names(data1) != "id"], col_norm)
})
```

```{r, out.width = "800px", echo=FALSE}
knitr::include_graphics("images/profvis4.jpg")
```

Now we have code that’s not only about 8x faster than our original – it’s shorter and more elegant as well. Not bad! The profiler data helped us to identify performance bottlenecks, and understanding of the underlying data structures allowed us to approach the problem in a more efficient way.

# Example 2

This example addresses some more advanced issues. This time, it will be hard to directly see the causes of slowness, but we will be able to see some of their side-effects, most notably the side-effects from large amounts of memory allocation.

Suppose you have a data frame that contains a column for which you’d like to take a cumulative sum (and you don’t know about R’s built-in `cumsum` function). Here’s one way to do it:

```{r eval=FALSE}
profvis({
  data <- data.frame(value = runif(5e4))

  data$sum[1] <- data$value[1]
  for (i in seq(2, nrow(data))) {
    data$sum[i] <- data$sum[i-1] + data$value[i]
  }
})
```

```{r, out.width = "800px", echo=FALSE}
knitr::include_graphics("images/profvis5.jpg")
```

This takes over 12 seconds to calculate the cumulative sum of 50,000 items. That’s pretty slow for a computer program. Looking at the `profvis` visualization, we can see a number of notable features:

- Almost all the time is spent in one line of code, line 6. Although this is just one line of code, many different functions that are called on that line.
- That line also results in a large amount of memory being allocated and deallocated, which is somewhat suprising: just looking at the code, that line appears to just modify the data in-place, but that’s not actually what’s happening internally.
- In the flame graph, you’ll see that some of the flame graph blocks have the label `$`, which means that those samples were spent in the `$` function for indexing into an object (in R, the expression `x$y` is equivalent to `$(x, "y")`).

Because `$` is a generic function, it calls the corresponding method for the object, in this case `$.data.frame`. This function in turn calls `[[`, which calls `[[.data.frame`. (Zoom in to see this more clearly.)

Other flame graph cells have the label `$<-.` The usual syntax for calling this function is `x$y <- z`; this is equivalent to `$<- (x, "y", z)`. (Assignment with indexing, as in `x$y[i] <- z` is actually a bit more complicated, and it turns out that this is the cause of the excessive memory allocation and deallocation.)

Finally, many of the flame graph cells contain the entire expression from line 6. This can mean one of two things:

- R is currently evaluating the expression but is not inside another function call.
- R is in another function, but that function does not show up on the stack. (A number of R’s internal functions do not show up in the profiling data.)

This profiling data tells us that much of the time is spent in `$` and `$<-`. Maybe avoiding these functions entirely will speed things up. To do that, instead of operating on data frame columns, we can operate on temporary vectors. As it turns out, writing a function that takes a vector as input and returns a vector as output is not only convenient; it provides a natural way of creating temporary variables so that we can avoid calling `$` and  `$<-` in a loop.

```{r eval=FALSE}
profvis({
  csum <- function(x) {
    if (length(x) < 2) return(x)

    sum <- x[1]
    for (i in seq(2, length(x))) {
      sum[i] <- sum[i-1] + x[i]
    }
    sum
  }
  data$sum <- csum(data$value)
})
```

```{r, out.width = "800px", echo=FALSE}
knitr::include_graphics("images/profvis6.jpg")
```

Using this `csum` function, it takes about 20 ms, which is about 600x as fast as before.

It may appear that no functions are called from line 7, but that’s not quite true: that line also calls `[`, `[<-`, `-`, and `+`.

- The `[` and `[<-` functions don’t appear in the flame graph. They are internal R functions which contain C code to handle indexing into atomic vectors, and are not dispatched to methods. (Contrast this with the first version of the code, where `$` was dispatched to  `$.data.frame`).
- The `-` and `+` functions can show up in a flame graph, but they are very fast so the sampling profiler may or may not happen to take a sample when they’re on the call stack.

The code panel shows that there is still a large amount of memory being allocated in the `csum` function. In the flame graph. you probably have noticed the gray blocks labeled `<GC>`. These represent times where R is doing garbage collection – that is, when it is freeing (deallocating) chunks of memory that were allocated but no longer needed. _If R is spending a lot of time freeing memory, that suggests that R is also spending a lot of time allocating memory._ This is another common source of slowness in R code.

In the `csum` function, `sum` starts as a length-1 vector, and then grows, in a loop, to be the same length as `x`. Every time a vector grows, R allocates a new block of memory for the new, larger vector, and then copies the contents over. The memory allocated for the old vector is no longer needed, and will later be garbage collected.

To avoid all that memory allocation, copying, and garbage collection, _we can pre-allocate a correctly-sized vector for `sum`_. For this data, that will result in 49,999 fewer allocations, copies, and deallocations.

```{r eval=FALSE}
profvis({
  csum2 <- function(x) {
    if (length(x) < 2) return(x)

    sum <- numeric(length(x))  # Preallocate
    sum[1] <- x[1]
    for (i in seq(2, length(x))) {
      sum[i] <- sum[i-1] + x[i]
    }
    sum
  }
  data$sum <- csum2(data$value)
})
```

```{r, out.width = "800px", echo=FALSE}
knitr::include_graphics("images/profvis7.jpg")
```

This version of the code, with `csum2`, is around 60x faster than our original code, and requires almost no memory allocation. These performance improvements were possible by avoiding calls to `$` and `$<-`, and by avoiding unnecessary memory allocation and copying from growing a vector in a loop.

# Example 3 - Profiling a Shiny Application

In addition to R code, you can also profile Shiny applications. To do this, simply execute `runApp()` command inside of `profvis`. 

The Shiny application will launch and after interacting and closing the app a profile will be generated.

```{r eval=FALSE}
profvis({
  runExample(example = "06_tabsets", display.mode = "normal")
})
```
